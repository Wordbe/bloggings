# Spark 9 - Data Lake



# 최적화된 스토리지 중요성

데이터베이스 → 데이터 레이크 → 데이터 레이크하우스

- 확장 및 성능
- 트랜잭션 지원
- 다양한 데이터 형식 지원
- 다양한 워크로드 지원 (SQL, ETL, Streaming, ML, AI)
- 개방성 (표준 API)



# 데이터베이스

데이터베이스의 SQL 워크로드

- OLTP, 온라인 트랜잭션 처리 : 몇몇의 레코드 읽고 업데이트. 높은 동시성, 짧은 지연 시간, 간단한 쿼리
- OLAP, 온라인 분석 처리 : 정기 레포트. 다량의 레코드에 대한 높은 처리량 스캔 필요, 복잡한 쿼리(집계 및 조인)

스파크는 주로 OLAP 워크로드용으로 설계되었다.

## 데이터베이스의 한계

- 데이터 크기의 증가
- 분석의 다양성 증가
- 데이터베이스를 확장하는 데 비용이 너무 많이 듦
- 데이터베이스는 비SQL 기반 분석을 잘 지원하지 않음



# 데이터 레이크

- 분산 컴퓨팅 시스템에서 분산 스토리지 시스템을 분리한다. 각 시스템은 워크로드에 따라 필요한 확장을 할 수 있다.
- 모든 처리 엔진이 표준 API 를 사용하여 읽고 쓸 수 있도록 오픈 형식의 파일로 저자된다.
- 스토리지 시스템
- 파일 형식 (정형, 비정형)
- 처리엔진

## 데이터 레이크의 한계

- 트랜잭션에 대한 보증이 부족. ACID 보증하지 않음
  - 원자성과 독립성, 일관성





# 레이크하우스

- 트랜잭션 지원한다. ACID 보장
- 스키마 적용, 거버넌스 및 검사 메커니즘 존재
- 오픈 형식의 다양한 데이터 유형 지원
- 다양한 워크로드 지원
- upsert 및 삭제 지원
  - CDC (change-data-capture), SCD(slowly changing dimension) 가능
- 데이터 거버넌스 : 데이터의 무결성에 대해 추론하고 정책 준수를 위해 모든 데이터 변경사항 감사

## 아파치 후디 Apache Hudi

- 키/값 데이터에 대한 증분값의 업서트, 삭제를 위해 설계된 데이터 저장 형식
- 열, 행 기반 둘 다 지원

## 아파치 아이스버그

- 단일 테이블에서 페타바이트까지 증가할 수 있는 확장성가짐.

## 델타 레이크 Delta lake

- 트랜잭션 보증, 오픈 데이터 저장 형식. 스트리밍과 비슷
- 테이블 로드
  - delta 사용하려면 `format("parquet") 대신 ` `format("delta")` 만 변경하면 됨.
- 테이블에 데이터 스트림 로드
  - 다른 형식과 마찬 가지로 정형화 스트리밍이 종단 간 일회 처리 보장
  - 배치 처리 및 스트리밍 작업 모두에 동일한 테이블에 쓰기 가능
  - 여러 스트리밍 작업에서 동일한 테이블에 데이터 추가 가능
  - 동시 쓰기에서도 ACID 보장 제공
- 데이터 손상을 방지하기 위해 쓰기 시 스키마 적용
- 변경이 생기면 스키마 수용 가능 - `option("mergeSchema", "true")`
- 데이터 업데이트 쉽게 가능
- 사용자 관련 데이터 삭제
- 업서트 : `deltaTable.alias("t").merge(loanUpdates.alias("s"), "t.loan_id = s.loan_id" ).whenMatched().updateAll().whenNotMatched().insertAll().execute()`
- 작업 내역으로 데이터 변경 검사
  - `deltaTable.history().show()`
- 시간 탐색을 사용하여 테이블의 이전 스냅샷 쿼리 (시간관리, 버전관리 가능)
  - `spark.read.format("delta").option("timestampAsOf", "2023-01-01").load(deltaPath)`
  - `spark.read.format("delta").option("versionAsOf", "4").load(deltaPath)`